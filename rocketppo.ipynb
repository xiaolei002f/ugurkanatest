{"cells":[{"cell_type":"code","metadata":{"id":"jf41_a7vlZZk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":425},"executionInfo":{"status":"ok","timestamp":1593268353711,"user_tz":-180,"elapsed":2527,"user":{"displayName":"uğurkan ateş","photoUrl":"","userId":"03432754458149960388"}},"outputId":"db9accd5-cc56-4cb4-e6bd-68e8a80efc73"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)\n","\n","from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n","  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n","  print('re-execute this cell.')\n","else:\n","  print('You are using a high-RAM runtime!')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Sat Jun 27 14:32:32 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   48C    P0    32W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n","Your runtime has 27.4 gigabytes of available RAM\n","\n","You are using a high-RAM runtime!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qZv83CkLLEHd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":587},"executionInfo":{"status":"ok","timestamp":1593268364366,"user_tz":-180,"elapsed":12259,"user":{"displayName":"uğurkan ateş","photoUrl":"","userId":"03432754458149960388"}},"outputId":"81386f35-cb70-445c-abb0-4e1d80d1023f"},"source":["!pip install ptan tensorboardX box2D\n","!pip uninstall pyglet- y\n","!pip install pyglet==v1.3.2"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: ptan in /usr/local/lib/python3.6/dist-packages (0.6)\n","Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (2.0)\n","Requirement already satisfied: box2D in /usr/local/lib/python3.6/dist-packages (2.3.10)\n","Requirement already satisfied: atari-py in /usr/local/lib/python3.6/dist-packages (from ptan) (0.2.6)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from ptan) (4.1.2.30)\n","Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from ptan) (0.17.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ptan) (1.18.5)\n","Requirement already satisfied: torch==1.3.0 in /usr/local/lib/python3.6/dist-packages (from ptan) (1.3.0)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->ptan) (1.4.1)\n","Collecting pyglet<=1.5.0,>=1.4.0\n","  Using cached https://files.pythonhosted.org/packages/70/ca/20aee170afe6011e295e34b27ad7d7ccd795faba581dd3c6f7cec237f561/pyglet-1.5.0-py2.py3-none-any.whl\n","Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->ptan) (1.3.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (47.3.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->ptan) (0.16.0)\n","Installing collected packages: pyglet\n","  Found existing installation: pyglet 1.3.2\n","    Uninstalling pyglet-1.3.2:\n","      Successfully uninstalled pyglet-1.3.2\n","Successfully installed pyglet-1.5.0\n","\u001b[31mERROR: Invalid requirement: 'pyglet-'\u001b[0m\n","Collecting pyglet==v1.3.2\n","  Using cached https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet==v1.3.2) (0.16.0)\n","\u001b[31mERROR: gym 0.17.2 has requirement pyglet<=1.5.0,>=1.4.0, but you'll have pyglet 1.3.2 which is incompatible.\u001b[0m\n","Installing collected packages: pyglet\n","  Found existing installation: pyglet 1.5.0\n","    Uninstalling pyglet-1.5.0:\n","      Successfully uninstalled pyglet-1.5.0\n","Successfully installed pyglet-1.3.2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yUOjoN6LLFhT","colab_type":"text"},"source":["**Python Packages**"]},{"cell_type":"code","metadata":{"id":"6dy1UcKPLIY7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":158},"executionInfo":{"status":"ok","timestamp":1593268368126,"user_tz":-180,"elapsed":13851,"user":{"displayName":"uğurkan ateş","photoUrl":"","userId":"03432754458149960388"}},"outputId":"e9138a49-7999-458e-a566-e76b5d9600ec"},"source":["!git clone https://github.com/ugurkanates/SpaceXReinforcementLearning.git\n","%cd SpaceXReinforcementLearning\n","!ls"],"execution_count":3,"outputs":[{"output_type":"stream","text":["fatal: destination path 'SpaceXReinforcementLearning' already exists and is not an empty directory.\n","/content/SpaceXReinforcementLearning\n","02_play.py\t     examples\t\tlibSAC\t\t runs\t       tests\n","adamPPO\t\t     fak.dat\t\tLICENSE.md\t sac_train.py  vendor\n","bin\t\t     gym\t\tppo.py\t\t saves\n","CODE_OF_CONDUCT.rst  keyboard_agent.py\tpy.Dockerfile\t scripts\n","d4pg.py\t\t     lib\t\tREADME.md\t setup.py\n","docs\t\t     libDDPG\t\troketvideotest2  test.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bRV8nSFOblN3","colab_type":"text"},"source":["google upload test\n"]},{"cell_type":"code","metadata":{"id":"WIIwmrTkblm0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1593268368129,"user_tz":-180,"elapsed":11615,"user":{"displayName":"uğurkan ateş","photoUrl":"","userId":"03432754458149960388"}},"outputId":"5a2b36f5-fd2f-4d7d-b356-4e473d650feb"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-n6umBeVPJDJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":496},"executionInfo":{"status":"ok","timestamp":1593268390542,"user_tz":-180,"elapsed":18842,"user":{"displayName":"uğurkan ateş","photoUrl":"","userId":"03432754458149960388"}},"outputId":"574ad03c-16da-4096-8da1-09534dd9233a"},"source":["!apt install python-opengl\n","!apt install ffmpeg\n","!apt install xvfb\n","!pip3 install pyvirtualdisplay\n","\n","# Virtual display\n","from pyvirtualdisplay import Display\n","\n","virtual_display = Display(visible=0, size=(1400, 900))\n","virtual_display.start()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","python-opengl is already the newest version (3.1.0+dfsg-1).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-440\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 59 not upgraded.\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","ffmpeg is already the newest version (7:3.4.6-0ubuntu0.18.04.1).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-440\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 59 not upgraded.\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","xvfb is already the newest version (2:1.19.6-1ubuntu4.4).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-440\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 59 not upgraded.\n","Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (1.3.2)\n","Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.3)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<pyvirtualdisplay.display.Display at 0x7f832a1d45f8>"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"ol2ZMBoJNfdD","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593268426895,"user_tz":-180,"elapsed":1027,"user":{"displayName":"uğurkan ateş","photoUrl":"","userId":"03432754458149960388"}}},"source":["#!/usr/bin/env python3\n","import os\n","import math\n","import ptan\n","import time\n","import gym\n","import argparse\n","from tensorboardX import SummaryWriter\n","\n","from lib import model, test_net, calc_logprob\n","\n","import numpy as np\n","import torch\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","RETURN_TRAIN = False\n","# You have to manually change this for your drive , folders has to exists in your Drive.\n","actor_saved_net = \"/content/gdrive/My Drive/colab_model/rocket/ppo/run1/actor.dat\"\n","critic_saved_net = \"/content/gdrive/My Drive/colab_model/rocket/ppo/run1/critic.dat\"\n","\n","ENV_ID = \"RocketLander-v0\"\n","GAMMA = 0.99\n","GAE_LAMBDA = 0.95\n","\n","TRAJECTORY_SIZE = 4097\n","LEARNING_RATE_ACTOR = 1e-5\n","LEARNING_RATE_CRITIC = 1e-4\n","\n","PPO_EPS = 0.2\n","PPO_EPOCHES = 10\n","PPO_BATCH_SIZE = 256\n","\n","TEST_ITERS = 100000\n","test_count = 0\n","\n","def calc_adv_ref(trajectory, net_crt, states_v, device=\"cpu\"):\n","    \"\"\"\n","    By trajectory calculate advantage and 1-step ref value\n","    :param trajectory: trajectory list\n","    :param net_crt: critic network\n","    :param states_v: states tensor\n","    :return: tuple with advantage numpy array and reference values\n","    \"\"\"\n","    values_v = net_crt(states_v)\n","    values = values_v.squeeze().data.cpu().numpy()\n","    # generalized advantage estimator: smoothed version of the advantage\n","    last_gae = 0.0\n","    result_adv = []\n","    result_ref = []\n","    for val, next_val, (exp,) in zip(reversed(values[:-1]),\n","                                     reversed(values[1:]),\n","                                     reversed(trajectory[:-1])):\n","        if exp.done:\n","            delta = exp.reward - val\n","            last_gae = delta\n","        else:\n","            delta = exp.reward + GAMMA * next_val - val\n","            last_gae = delta + GAMMA * GAE_LAMBDA * last_gae\n","        result_adv.append(last_gae)\n","        result_ref.append(last_gae + val)\n","\n","    adv_v = torch.FloatTensor(list(reversed(result_adv)))\n","    ref_v = torch.FloatTensor(list(reversed(result_ref)))\n","    return adv_v.to(device), ref_v.to(device)\n","\n","\n","def main():\n","    test_count = 0\n","    name = \"pporun1\"\n","    device = torch.device(\"cuda\")\n","\n","    save_path = os.path.join(\"saves\", \"ppo-\" + name)\n","    os.makedirs(save_path, exist_ok=True)\n","\n","    env = gym.make(ENV_ID)\n","    directory = './roketvideotest2/'\n","    env = gym.wrappers.Monitor(env, directory,force=True)\n","    test_env = gym.make(ENV_ID)\n","\n","    net_act = model.ModelActor(env.observation_space.shape[0], env.action_space.shape[0]).to(device)\n","    net_crt = model.ModelCritic(env.observation_space.shape[0]).to(device)\n","    print(net_act)\n","    print(net_crt)\n","\n","    writer = SummaryWriter(comment=\"-ppo_\" + name)\n","    agent = model.AgentA2C(net_act, device=device)\n","    exp_source = ptan.experience.ExperienceSource(env, agent, steps_count=1)\n","\n","    opt_act = optim.Adam(net_act.parameters(), lr=LEARNING_RATE_ACTOR)\n","    opt_crt = optim.Adam(net_crt.parameters(), lr=LEARNING_RATE_CRITIC)\n","\n","    trajectory = []\n","    best_reward = None\n","    with ptan.common.utils.RewardTracker(writer) as tracker:\n","        for step_idx, exp in enumerate(exp_source):\n","            rewards_steps = exp_source.pop_rewards_steps()\n","            if rewards_steps:\n","                rewards, steps = zip(*rewards_steps)\n","                writer.add_scalar(\"episode_steps\", np.mean(steps), step_idx)\n","                tracker.reward(np.mean(rewards), step_idx)\n","\n","            if step_idx % TEST_ITERS == 0:\n","                if(test_count == 15):\n","                  print(\"resetting best_wards\")\n","                  best_reward = None\n","                test_count += 1\n","                ts = time.time()\n","                rewards, steps = test_net(net_act, test_env, device=device)\n","                print(\"Test done in %.2f sec, reward %.3f, steps %d\" % (\n","                    time.time() - ts, rewards, steps))\n","                writer.add_scalar(\"test_reward\", rewards, step_idx)\n","                writer.add_scalar(\"test_steps\", steps, step_idx)\n","                if best_reward is None or best_reward < rewards:\n","                    if best_reward is not None:\n","                        print(\"Best reward updated: %.3f -> %.3f\" % (best_reward, rewards))\n","                        name = \"best_%+.3f_%d.dat\" % (rewards, step_idx)\n","                        fname_actor = os.path.join(save_path,\"actor\"+name)\n","                        fname_critic = os.path.join(save_path,\"critic\"+name)\n","                        torch.save(net_act.state_dict(), fname_actor)\n","                        torch.save(net_crt.state_dict(),fname_critic)\n","                        !cp fname_actor \"/content/gdrive/My Drive/colab_model/rocket/ppo/run1/\" # copies file to google drive\n","                        !cp fname_critic \"/content/gdrive/My Drive/colab_model/rocket/ppo/run1/\" # copies file to google drive\n","                    best_reward = rewards\n","\n","            trajectory.append(exp)\n","            if len(trajectory) < TRAJECTORY_SIZE:\n","                continue\n","\n","            traj_states = [t[0].state for t in trajectory]\n","            traj_actions = [t[0].action for t in trajectory]\n","            traj_states_v = torch.FloatTensor(traj_states)\n","            traj_states_v = traj_states_v.to(device)\n","            traj_actions_v = torch.FloatTensor(traj_actions)\n","            traj_actions_v = traj_actions_v.to(device)\n","            traj_adv_v, traj_ref_v = calc_adv_ref(\n","                trajectory, net_crt, traj_states_v, device=device)\n","            mu_v = net_act(traj_states_v)\n","            old_logprob_v = calc_logprob(\n","                mu_v, net_act.logstd, traj_actions_v)\n","\n","            # normalize advantages\n","            traj_adv_v = traj_adv_v - torch.mean(traj_adv_v)\n","            traj_adv_v /= torch.std(traj_adv_v)\n","\n","            # drop last entry from the trajectory, an our adv and ref value calculated without it\n","            trajectory = trajectory[:-1]\n","            old_logprob_v = old_logprob_v[:-1].detach()\n","\n","            sum_loss_value = 0.0\n","            sum_loss_policy = 0.0\n","            count_steps = 0\n","\n","            for epoch in range(PPO_EPOCHES):\n","                for batch_ofs in range(0, len(trajectory),\n","                                       PPO_BATCH_SIZE):\n","                    batch_l = batch_ofs + PPO_BATCH_SIZE\n","                    states_v = traj_states_v[batch_ofs:batch_l]\n","                    actions_v = traj_actions_v[batch_ofs:batch_l]\n","                    batch_adv_v = traj_adv_v[batch_ofs:batch_l]\n","                    batch_adv_v = batch_adv_v.unsqueeze(-1)\n","                    batch_ref_v = traj_ref_v[batch_ofs:batch_l]\n","                    batch_old_logprob_v = \\\n","                        old_logprob_v[batch_ofs:batch_l]\n","\n","                    # critic training\n","                    opt_crt.zero_grad()\n","                    value_v = net_crt(states_v)\n","                    loss_value_v = F.mse_loss(\n","                        value_v.squeeze(-1), batch_ref_v)\n","                    loss_value_v.backward()\n","                    opt_crt.step()\n","\n","                    # actor training\n","                    opt_act.zero_grad()\n","                    mu_v = net_act(states_v)\n","                    logprob_pi_v = calc_logprob(\n","                        mu_v, net_act.logstd, actions_v)\n","                    ratio_v = torch.exp(\n","                        logprob_pi_v - batch_old_logprob_v)\n","                    surr_obj_v = batch_adv_v * ratio_v\n","                    c_ratio_v = torch.clamp(ratio_v,\n","                                            1.0 - PPO_EPS,\n","                                            1.0 + PPO_EPS)\n","                    clipped_surr_v = batch_adv_v * c_ratio_v\n","                    loss_policy_v = -torch.min(\n","                        surr_obj_v, clipped_surr_v).mean()\n","                    loss_policy_v.backward()\n","                    opt_act.step()\n","\n","                    sum_loss_value += loss_value_v.item()\n","                    sum_loss_policy += loss_policy_v.item()\n","                    count_steps += 1\n","\n","            trajectory.clear()\n","            writer.add_scalar(\"advantage\", traj_adv_v.mean().item(), step_idx)\n","            writer.add_scalar(\"values\", traj_ref_v.mean().item(), step_idx)\n","            writer.add_scalar(\"loss_policy\", sum_loss_policy / count_steps, step_idx)\n","            writer.add_scalar(\"loss_value\", sum_loss_value / count_steps, step_idx)\n","\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pz8FRqzXltLK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":370},"executionInfo":{"status":"error","timestamp":1593276488779,"user_tz":-180,"elapsed":9872,"user":{"displayName":"uğurkan ateş","photoUrl":"","userId":"03432754458149960388"}},"outputId":"c5c0d54b-4eb7-4c7e-9581-03b59fecead1"},"source":["main()"],"execution_count":9,"outputs":[{"output_type":"stream","text":["6038648: done 10409 episodes, mean reward -3.829, speed 1028.31 f/s\n","6039018: done 10410 episodes, mean reward -3.826, speed 255.20 f/s\n","6040151: done 10412 episodes, mean reward -3.797, speed 1042.61 f/s\n","6041739: done 10414 episodes, mean reward -3.797, speed 1024.98 f/s\n","6043282: done 10416 episodes, mean reward -3.820, speed 599.09 f/s\n","6044692: done 10418 episodes, mean reward -3.815, speed 1007.76 f/s\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-8-1dbfa0a0b35d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mbest_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mptan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRewardTracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtracker\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstep_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0mrewards_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_source\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop_rewards_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrewards_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ptan/experience.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m                     \u001b[0mnext_state_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                     \u001b[0mnext_state_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mis_done\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/SpaceXReinforcementLearning/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/SpaceXReinforcementLearning/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/SpaceXReinforcementLearning/gym/envs/box2d/rocket_lander.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m0.25\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mFPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;31m# REWARD -------------------------------------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mclip\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mclip\u001b[0;34m(a, a_min, a_max, out, **kwargs)\u001b[0m\n\u001b[1;32m   2082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m     \"\"\"\n\u001b[0;32m-> 2084\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_clip\u001b[0;34m(a, min, max, out, casting, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_clip_dep_is_byte_swapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_clip_dep_is_byte_swapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0musing_deprecated_nan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0m_clip_dep_is_scalar_nan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0mmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0musing_deprecated_nan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_clip_dep_is_scalar_nan\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_clip_dep_is_scalar_nan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# guarded to protect circular imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromnumeric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}],"metadata":{"colab":{"name":"rocketppo.ipynb","provenance":[{"file_id":"1U93rCtj22EyciL29RdspBKLIWK5YLGz4","timestamp":1593266621829}],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyPXdOnbcprmSOedIICGohtk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}